{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import time\n",
    "xformatter = mdates.DateFormatter('%H:%M')  # for time axis plots\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pickle import load,dump\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from keras.models import Sequential,tf\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Bidirectional, BatchNormalization, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from keras.backend import clear_session\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_TIME='%Y-%m-%d %H:%M:%S'\n",
    "START_TIME_DAY = time(5,0,0)\n",
    "END_TIME_DAY = time(18,30,0)\n",
    "TIME_1_STEP = 15 # minute\n",
    "step_lag_1_day = 24*60//TIME_1_STEP\n",
    "steps_2hours = 60*2//TIME_1_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = ['linear','relu', 'tanh','sigmoid']\n",
    "optimizer_list = ['adam', 'rmsprop','sgd','Adamax']\n",
    "range_neurons_lstm = [8,100]\n",
    "range_neurons_cnn = [8,100]\n",
    "selectors = []\n",
    "range_pool_size = [1,2]\n",
    "range_kernel_size = [1,3]\n",
    "algorithm_param = {\n",
    "    'max_num_iteration': 5,\n",
    "    'population_size': 5,\n",
    "    'mutation_probability': 0.1,\n",
    "    'elit_ratio': 0.2,\n",
    "    'crossover_probability': 0.5,\n",
    "    'parents_portion': 0,\n",
    "    'crossover_type': 'uniform',\n",
    "    'max_iteration_without_improv': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhap du lieu train data\n",
    "def import_train_data(path_file_train):\n",
    "    df = pd.read_csv(path_file_train)\n",
    "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp']\n",
    "                                      )\n",
    "    df = df.set_index('TimeStamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_df(df, resample_time, time_col='TimeStamp'):\n",
    "    \"\"\"\n",
    "    resample_time: `minute`\n",
    "    \"\"\"\n",
    "    resample_df = df.copy()\n",
    "    if resample_time >= 30:\n",
    "        resample_df = resample_df.set_index(\n",
    "            resample_df[time_col] - to_offset(str(resample_time//2)+\"min\"))\n",
    "    resample_df = resample_df.resample(str(resample_time)+'min', label='right').mean()\n",
    "    return resample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plant = import_train_data('./LN2_training.csv')\n",
    "all_plant = resample_df(all_plant, resample_time = 15, time_col='TimeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, val data theo ti le 8/2\n",
    "def train_valid_split(df, split_ratio=[0.8, 0.2]):\n",
    "    train_ratio, valid_ratio = split_ratio\n",
    "    assert train_ratio + valid_ratio  == 1.0\n",
    "    n_df = len(df)\n",
    "    # Train / Validation  Split\n",
    "    train_split = int(n_df * train_ratio)\n",
    "    valid_split = int(n_df * (train_ratio + valid_ratio))\n",
    "\n",
    "    train = df[:train_split]\n",
    "    val = df[train_split:valid_split]\n",
    "\n",
    "    print(f'Train set: {len(train)} ')\n",
    "    print(f'Validation set: {len(val)} ')\n",
    "\n",
    "    return train, val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_step_1hour(df):\n",
    "    \"\"\"\n",
    "    Get number step of 1 hours\n",
    "    \"\"\"\n",
    "    step_hours = None\n",
    "    if type(df.index) == pd.core.indexes.datetimes.DatetimeIndex:\n",
    "        time_1step = int((df.index[1] - df.index[0]) /\n",
    "                         np.timedelta64(1, 'm'))  # minute\n",
    "        step_hours = 60 // time_1step\n",
    "    return step_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_supervised(dt, num_pre_around=5, num_day_pre=3):\n",
    "    step_lag_1_day = num_step_1hour(dt)*24\n",
    "    dt_lag = pd.DataFrame()\n",
    "    for col in dt.columns:\n",
    "        for day_pre in range(num_pre_around+1):\n",
    "            if day_pre == 0:\n",
    "                dt_lag[col+'(t)'] = dt[col]\n",
    "            else:\n",
    "                dt_lag[col+'(t-'+str(day_pre)+')'] = dt[col].shift(day_pre)\n",
    "        for day_pre in range(1, num_day_pre):\n",
    "            step_lag = step_lag_1_day*day_pre\n",
    "            for lag in range(num_pre_around+1):\n",
    "                dt_lag[col+'(t-'+str(step_lag+lag) +\n",
    "                       ')'] = dt[col].shift(step_lag+lag)\n",
    "                dt_lag[col+'(t-'+str(step_lag-lag) +\n",
    "                       ')'] = dt[col].shift(step_lag-lag)\n",
    "    dt_lag = dt_lag.dropna()\n",
    "    dt_lag['hour'] = dt_lag.index.hour\n",
    "    dt_lag['day'] = dt_lag.index.day\n",
    "    dt_lag['day_of_week'] = dt_lag.index.dayofweek\n",
    "    dt_lag['month'] = dt_lag.index.month\n",
    "    dt_lag['day_of_year'] = dt_lag.index.dayofyear\n",
    "    return dt_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_Kbest(train,score_f):\n",
    "    X_select = train[train.columns[1:]]\n",
    "    y_select = train['TotW(t)']\n",
    "    bestfeatures = SelectKBest(score_func=score_f, k='all')\n",
    "    fit = bestfeatures.fit(X_select,y_select)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X_select.columns)\n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  \n",
    "    featureScores = featureScores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(train, val, GA_result):\n",
    "    train_X, train_y = train.values[:, 1:], train.values[:, :1]\n",
    "    # train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n",
    "    \n",
    "    val_X, val_y = val.values[:, 1:], val.values[:, :1]\n",
    "    # val_X = val_X.reshape(val_X.shape[0], 1, val_X.shape[1])\n",
    "    train_X, train_y = create_window_data(train_X, train_y, window_size)\n",
    "    val_X, val_y = create_window_data(val_X, val_y, window_size)\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "\n",
    "    model.add(Conv1D(filters=GA_result['neurons_cnn'], kernel_size=GA_result['kernel_size'], \n",
    "                                                        padding='valid',\n",
    "                                                        activation=GA_result['activations'], \n",
    "                                                        input_shape=(train_X.shape[1],train_X.shape[2])))\n",
    "    model.add(MaxPooling1D(pool_size=GA_result['pool_size'], padding='SAME'))                                                  \n",
    "    model.add(Flatten())\n",
    "    model.add(tf.keras.layers.Reshape((1, model.output_shape[1])))\n",
    "    model.add(\n",
    "        Bidirectional(LSTM(GA_result['neurons_lstm'],\n",
    "             activation=GA_result['activations'],return_sequences=True)))\n",
    "    \n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))   \n",
    "    # model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=GA_result['optimizers'],\n",
    "                  metrics=['mse', 'mae', 'cosine_proximity'])\n",
    "    # fit network\n",
    "    history = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        epochs=100,\n",
    "                        batch_size=50,\n",
    "                        validation_data=(val_X, val_y),\n",
    "                        verbose=1,\n",
    "                        shuffle=False,\n",
    "                        callbacks=EarlyStopping(monitor='val_loss',\n",
    "                                                patience=15,\n",
    "                                                restore_best_weights=True))\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    fig.suptitle('Loss', y=0.93)\n",
    "    ax.plot(history.history['mae'], label='train')\n",
    "    ax.plot(history.history['val_mae'], label='val')\n",
    "    ax.set_title('mae')\n",
    "    ax.legend(loc='upper right')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_path(path):\n",
    "    if os.path.isdir(path) is False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_func(train, val, path_scale):\n",
    "    get_or_create_path(path_scale)\n",
    "    train_scale_df = pd.DataFrame(index=train.index)\n",
    "    val_scale_df = pd.DataFrame(index=val.index)\n",
    "    \n",
    "    for col in train.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scale_df[col] = scaler.fit_transform(train[col].values.reshape(-1,1))[:,0]\n",
    "        val_scale_df[col] = scaler.transform(val[col].values.reshape(-1,1))[:,0]\n",
    "        pickle.dump(scaler, open(path_scale + col+'.pkl','ab+'))\n",
    "    return train_scale_df,val_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GA_result(ga_model):\n",
    "    best_neurons_lstm = (int(ga_model.best_variable[0]))\n",
    "    best_neurons_cnn = (int(ga_model.best_variable[1]))\n",
    "    best_activation = (int(ga_model.best_variable[2]))\n",
    "    best_optimizer = (int(ga_model.best_variable[3]))\n",
    "    best_pool_size = (int(ga_model.best_variable[4]))\n",
    "    best_kernel_size = (int(ga_model.best_variable[5]))\n",
    "    return {\n",
    "        'activations': activation_list[best_activation],\n",
    "        'optimizers': optimizer_list[best_optimizer],\n",
    "        'neurons_lstm': best_neurons_lstm,\n",
    "        'neurons_cnn': best_neurons_cnn,\n",
    "        'pool_size': best_pool_size,\n",
    "        'kernel_size': best_kernel_size,\n",
    "        'num_feature': int(ga_model.best_variable[6])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_path(path):\n",
    "    if os.path.isdir(path) is False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_func(train, val, path_scale):\n",
    "    get_or_create_path(path_scale)\n",
    "    train_scale_df = pd.DataFrame(index=train.index)\n",
    "    val_scale_df = pd.DataFrame(index=val.index)\n",
    "    \n",
    "    for col in train.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scale_df[col] = scaler.fit_transform(train[col].values.reshape(-1,1))[:,0]\n",
    "        val_scale_df[col] = scaler.transform(val[col].values.reshape(-1,1))[:,0]\n",
    "        pickle.dump(scaler, open(path_scale + col+'.pkl','ab+'))\n",
    "    return train_scale_df,val_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_solar, val_solar = train_valid_split(all_plant,split_ratio=[0.8, 0.2]) # split train/val\n",
    "train_scale, val_scale = scale_func(train_solar, val_solar, './LN2/')\n",
    "\n",
    "train_scaler_lag = make_data_supervised(train_scale)\n",
    "val_scaler_lag = make_data_supervised(val_scale)\n",
    "\n",
    "col_analysis = list(train_scaler_lag)\n",
    "train_solar = train_scaler_lag[col_analysis].copy()\n",
    "val_solar = val_scaler_lag[col_analysis].copy()\n",
    "\n",
    "featureScores = select_Kbest(train_solar[col_analysis],f_regression)\n",
    "# featureScores.to_csv('./LN2_4H_kbest.csv')\n",
    "\n",
    "train_X, train_y= train_solar.values[:, 1:],train_solar.values[:, :1]\n",
    "# train_X = train_X.reshape(train_X.shape[0] ,1, train_X.shape[1])\n",
    "val_X, val_y= val_solar.values[:, 1:],val_solar.values[:, :1]\n",
    "# val_X = val_X.reshape(val_X.shape[0], 1, val_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_data(train_X, train_y, window_size):\n",
    "    \"\"\"\n",
    "    Tạo window slide cho dự báo chuỗi thời gian bằng mô hình CNN-LSTM.\n",
    "\n",
    "    Tham số:\n",
    "    X: ndarray, đầu vào của chuỗi thời gian\n",
    "    y: ndarray, đầu ra của chuỗi thời gian\n",
    "    window_size: int, kích thước cửa sổ\n",
    "\n",
    "    Trả về:\n",
    "    X_window: ndarray, đầu vào được chia thành các cửa sổ\n",
    "    y_window: ndarray, đầu ra được chia thành các cửa sổ\n",
    "    \"\"\"\n",
    "    X_window, y_window = [], []\n",
    "    for i in range(len(train_X) - window_size):\n",
    "        X_window.append(train_X[i:i+window_size])\n",
    "        y_window.append(train_y[i+window_size])\n",
    "    return np.array(X_window), np.array(y_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gene):\n",
    "    g = [int(i) for i in gene]\n",
    "    train_X_ = train_X.copy()\n",
    "    val_X_ = val_X.copy()\n",
    "    train_y_ = train_y.copy()\n",
    "    val_y_ = val_y.copy()\n",
    "    selector = SelectKBest(score_func=f_regression, k=g[6])\n",
    "    train_X_ = train_X_.reshape(train_X_.shape[0], train_X_.shape[1])\n",
    "    val_X_ = val_X_.reshape(val_X_.shape[0], train_X_.shape[1])\n",
    "    train_X_ = selector.fit_transform(train_X_, train_y_)\n",
    "    val_X_ = selector.transform(val_X_)\n",
    "    train_X_, train_y_ = create_window_data(train_X, train_y, window_size)\n",
    "    val_X_, val_y_ = create_window_data(val_X, val_y, window_size)\n",
    "    \n",
    "    # train_X_ = train_X_.reshape(train_X_.shape[0],window_size,train_X_.shape[1])\n",
    "    # val_X_ = val_X_.reshape(val_X_.shape[0],window_size,val_X_.shape[1])\n",
    "    neurons_lstm = g[0]\n",
    "    neurons_cnn = g[1]\n",
    "    activation = activation_list[g[2]]\n",
    "    optimizer = optimizer_list[g[3]] \n",
    "    pool_size = g[4]\n",
    "    kernel_size = g[5]\n",
    "    print('\\nNumber of neurons LSTM: ', neurons_lstm,\n",
    "            ', Number of neurons CNN:',neurons_cnn,\n",
    "            ', activation function: ', activation,\n",
    "            ', optimizer function: ', optimizer,\n",
    "            ', features: ', g[6] ,\n",
    "            ', pool_size:', pool_size,\n",
    "            ', kernel_size', kernel_size)\n",
    "    clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=neurons_cnn, kernel_size= kernel_size, activation=activation, \n",
    "                                            padding='valid',\n",
    "                                            input_shape=(train_X_.shape[1],train_X_.shape[2]))) \n",
    "    model.add(MaxPooling1D(pool_size=pool_size, padding='SAME'))   \n",
    "    model.add(Flatten())   \n",
    "    print(model.output_shape)\n",
    "    model.add(tf.keras.layers.Reshape((1, model.output_shape[1])))                                             \n",
    "    model.add(Bidirectional(LSTM(neurons_lstm,activation=activation,\n",
    "                                    return_sequences=True)))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))   \n",
    "    # model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer=optimizer)\n",
    "    history = model.fit(train_X_, train_y_,validation_data=(val_X_, val_y_),callbacks = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True), \n",
    "        epochs=100, batch_size=50, verbose=0,shuffle=False)\n",
    "    print('val_loss: ', min(history.history['val_loss']))\n",
    "    return min(history.history['val_loss'])\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varbound = np.array([range_neurons_lstm, range_neurons_cnn,[0, 3], [0, 3] , \n",
    "                        range_pool_size,range_kernel_size ,[1, train_X.shape[1]]])\n",
    "ga_model = ga(function=evaluate,\n",
    "            dimension=7,\n",
    "            variable_type='int',\n",
    "            function_timeout=10000,\n",
    "            variable_boundaries=varbound,\n",
    "            convergence_curve=False,\n",
    "            algorithm_parameters=algorithm_param)\n",
    "ga_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_result = GA_result(ga_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature = list(featureScores['Specs'][:ga_result['num_feature']])\n",
    "\n",
    "train_solar = train_scaler_lag[['TotW(t)']+best_feature]\n",
    "val_solar = val_scaler_lag[['TotW(t)']+best_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(train_solar,val_solar,ga_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27870e2cba32afb35df83c15876cb6bec71e23ef70eb52b377eee57487520364"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
