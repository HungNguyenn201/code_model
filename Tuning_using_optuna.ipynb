{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import time\n",
    "xformatter = mdates.DateFormatter('%H:%M')  # for time axis plots\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "style.use('seaborn-whitegrid')\n",
    "\n",
    "import optuna\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pickle import load,dump\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Bidirectional, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from keras.backend import clear_session\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_TIME='%Y-%m-%d %H:%M:%S'\n",
    "START_TIME_DAY = time(5,0,0)\n",
    "END_TIME_DAY = time(18,30,0)\n",
    "TIME_1_STEP = 15 # minute\n",
    "step_lag_1_day = 24*60//TIME_1_STEP\n",
    "steps_2hours = 60*2//TIME_1_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = ['linear','relu', 'tanh','sigmoid']\n",
    "optimizer_list = ['adam', 'rmsprop','sgd','Adamax']\n",
    "range_neurons_lstm = [8,80]\n",
    "selectors = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nhap du lieu train data\n",
    "def import_train_data(path_file_train):\n",
    "    df = pd.read_csv(path_file_train)\n",
    "    df['TimeStamp'] = pd.to_datetime(df['TimeStamp']\n",
    "                                      )\n",
    "    df = df.set_index('TimeStamp')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_df(df, resample_time, time_col='TimeStamp'):\n",
    "    \"\"\"\n",
    "    resample_time: `minute`\n",
    "    \"\"\"\n",
    "    resample_df = df.copy()\n",
    "    if resample_time >= 30:\n",
    "        resample_df = resample_df.set_index(\n",
    "            resample_df[time_col] - to_offset(str(resample_time//2)+\"min\"))\n",
    "    resample_df = resample_df.resample(str(resample_time)+'min', label='right').mean()\n",
    "    return resample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plant = import_train_data('./LN2_training.csv')\n",
    "all_plant = resample_df(all_plant, resample_time = 15, time_col='TimeStamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, val data theo ti le 8/2\n",
    "def train_valid_split(df, split_ratio=[0.8, 0.2]):\n",
    "    train_ratio, valid_ratio = split_ratio\n",
    "    assert train_ratio + valid_ratio  == 1.0\n",
    "    n_df = len(df)\n",
    "    # Train / Validation  Split\n",
    "    train_split = int(n_df * train_ratio)\n",
    "    valid_split = int(n_df * (train_ratio + valid_ratio))\n",
    "\n",
    "    train = df[:train_split]\n",
    "    val = df[train_split:valid_split]\n",
    "\n",
    "    print(f'Train set: {len(train)} ')\n",
    "    print(f'Validation set: {len(val)} ')\n",
    "\n",
    "    return train, val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_step_1hour(df):\n",
    "    \"\"\"\n",
    "    Get number step of 1 hours\n",
    "    \"\"\"\n",
    "    step_hours = None\n",
    "    if type(df.index) == pd.core.indexes.datetimes.DatetimeIndex:\n",
    "        time_1step = int((df.index[1] - df.index[0]) /\n",
    "                         np.timedelta64(1, 'm'))  # minute\n",
    "        step_hours = 60 // time_1step\n",
    "    return step_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_supervised(dt, num_pre_around=5, num_day_pre=3):\n",
    "    step_lag_1_day = num_step_1hour(dt)*24\n",
    "    dt_lag = pd.DataFrame()\n",
    "    for col in dt.columns:\n",
    "        for day_pre in range(num_pre_around+1):\n",
    "            if day_pre == 0:\n",
    "                dt_lag[col+'(t)'] = dt[col]\n",
    "            else:\n",
    "                dt_lag[col+'(t-'+str(day_pre)+')'] = dt[col].shift(day_pre)\n",
    "        for day_pre in range(1, num_day_pre):\n",
    "            step_lag = step_lag_1_day*day_pre\n",
    "            for lag in range(num_pre_around+1):\n",
    "                dt_lag[col+'(t-'+str(step_lag+lag) +\n",
    "                       ')'] = dt[col].shift(step_lag+lag)\n",
    "                dt_lag[col+'(t-'+str(step_lag-lag) +\n",
    "                       ')'] = dt[col].shift(step_lag-lag)\n",
    "    dt_lag = dt_lag.dropna()\n",
    "    dt_lag['hour'] = dt_lag.index.hour\n",
    "    dt_lag['day'] = dt_lag.index.day\n",
    "    dt_lag['day_of_week'] = dt_lag.index.dayofweek\n",
    "    dt_lag['month'] = dt_lag.index.month\n",
    "    dt_lag['day_of_year'] = dt_lag.index.dayofyear\n",
    "    return dt_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_Kbest(train,score_f):\n",
    "    X_select = train[train.columns[1:]]\n",
    "    y_select = train['TotW(t)']\n",
    "    bestfeatures = SelectKBest(score_func=score_f, k='all')\n",
    "    fit = bestfeatures.fit(X_select,y_select)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(X_select.columns)\n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  \n",
    "    featureScores = featureScores.sort_values(by='Score', ascending=False)\n",
    "\n",
    "    return featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(train, val, OPTUNA_result):\n",
    "    train_X, train_y = train.values[:, 1:], train.values[:, :1]\n",
    "    train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n",
    "    val_X, val_y = val.values[:, 1:], val.values[:, :1]\n",
    "    val_X = val_X.reshape(val_X.shape[0], 1, val_X.shape[1])\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(\n",
    "        Bidirectional(LSTM(OPTUNA_result['neurons'],\n",
    "             activation=OPTUNA_result['activations'], \n",
    "             input_shape=(train_X.shape[1],train_X.shape[2]),\n",
    "             return_sequences=True\n",
    "             )))\n",
    "    # model.add(Bidirectional(LSTM(64)))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))   \n",
    "    # model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=OPTUNA_result['optimizers'],\n",
    "                  metrics=['mse', 'mae', 'cosine_proximity'])\n",
    "    # fit network\n",
    "    history = model.fit(train_X,\n",
    "                        train_y,\n",
    "                        epochs=50,\n",
    "                        batch_size=50,\n",
    "                        validation_data=(val_X, val_y),\n",
    "                        verbose=1,\n",
    "                        shuffle=False,\n",
    "                        callbacks=EarlyStopping(monitor='val_loss',\n",
    "                                                patience=15,\n",
    "                                                restore_best_weights=True))\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    fig.suptitle('Loss', y=0.93)\n",
    "    ax.plot(history.history['mae'], label='train')\n",
    "    ax.plot(history.history['val_mae'], label='val')\n",
    "    ax.set_title('mae')\n",
    "    ax.legend(loc='upper right')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_path(path):\n",
    "    if os.path.isdir(path) is False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_func(train, val, path_scale):\n",
    "    get_or_create_path(path_scale)\n",
    "    train_scale_df = pd.DataFrame(index=train.index)\n",
    "    val_scale_df = pd.DataFrame(index=val.index)\n",
    "    \n",
    "    for col in train.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scale_df[col] = scaler.fit_transform(train[col].values.reshape(-1,1))[:,0]\n",
    "        val_scale_df[col] = scaler.transform(val[col].values.reshape(-1,1))[:,0]\n",
    "        pickle.dump(scaler, open(path_scale + col+'.pkl','ab+'))\n",
    "    return train_scale_df,val_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_path(path):\n",
    "    if os.path.isdir(path) is False:\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_func(train, val, path_scale):\n",
    "    get_or_create_path(path_scale)\n",
    "    train_scale_df = pd.DataFrame(index=train.index)\n",
    "    val_scale_df = pd.DataFrame(index=val.index)\n",
    "    \n",
    "    for col in train.columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scale_df[col] = scaler.fit_transform(train[col].values.reshape(-1,1))[:,0]\n",
    "        val_scale_df[col] = scaler.transform(val[col].values.reshape(-1,1))[:,0]\n",
    "        pickle.dump(scaler, open(path_scale + col+'.pkl','ab+'))\n",
    "    return train_scale_df,val_scale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_solar, val_solar = train_valid_split(all_plant,split_ratio=[0.8, 0.2]) # split train/val\n",
    "train_scale, val_scale = scale_func(train_solar, val_solar, './LN2/')\n",
    "\n",
    "train_scaler_lag = make_data_supervised(train_scale)\n",
    "val_scaler_lag = make_data_supervised(val_scale)\n",
    "\n",
    "col_analysis = list(train_scaler_lag)\n",
    "train_solar = train_scaler_lag[col_analysis].copy()\n",
    "val_solar = val_scaler_lag[col_analysis].copy()\n",
    "\n",
    "featureScores = select_Kbest(train_solar[col_analysis],f_regression)\n",
    "# featureScores.to_csv('./LN2_4H_kbest.csv')\n",
    "\n",
    "train_X, train_y= train_solar.values[:, 1:],train_solar.values[:, :1]\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n",
    "val_X, val_y= val_solar.values[:, 1:],val_solar.values[:, :1]\n",
    "val_X = val_X.reshape(val_X.shape[0], 1, val_X.shape[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    train_X_ = train_X.copy()\n",
    "    val_X_ = val_X.copy()\n",
    "    train_y_ = train_y.copy()\n",
    "    val_y_ = val_y.copy()\n",
    "    \n",
    "\n",
    "    k_features = trial.suggest_int(\"k_features\", train_X_.shape[1], train_X_.shape[2])\n",
    "    selector = SelectKBest(score_func=f_regression, k=k_features)\n",
    "    train_X_ = train_X_.reshape(train_X_.shape[0], train_X_.shape[2])\n",
    "    val_X_ = val_X_.reshape(val_X_.shape[0], val_X_.shape[2])\n",
    "    train_X_ = selector.fit_transform(train_X_, train_y_)\n",
    "    val_X_ = selector.transform(val_X_)\n",
    "    \n",
    "    train_X_ = train_X_.reshape(train_X_.shape[0],1,train_X_.shape[1])\n",
    "    val_X_ = val_X_.reshape(val_X_.shape[0],1,val_X_.shape[1])\n",
    "    \n",
    "    n_neurons = trial.suggest_int(\"n_neurons\", 8, 80)\n",
    "    activation = trial.suggest_int(\"activation\", 0 , 3)\n",
    "    optimizer = trial.suggest_int(\"optimizer\", 0, 3)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print('\\nNumber of neurons: ', n_neurons,\n",
    "            ', activation function: ', activation_list[activation],\n",
    "            ', optimizer function: ', optimizer_list[optimizer],\n",
    "            ', features: ', k_features)\n",
    "    clear_session()\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "        \n",
    "    model.add(Bidirectional(LSTM(n_neurons, activation=activation_list[activation], \n",
    "                                        input_shape=(train_X_.shape[1], train_X_.shape[2]),\n",
    "                                        return_sequences=True)))\n",
    "\n",
    "#     model.add(Bidirectional(LSTM(64)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))   \n",
    "#     model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer=optimizer_list[optimizer])\n",
    "    history = model.fit(train_X_, train_y_,validation_data=(val_X_, val_y_),callbacks = EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=15,\n",
    "                        restore_best_weights=True), \n",
    "                        epochs=100, batch_size=50, verbose=0,shuffle=False)\n",
    "    print('val_loss: ', min(history.history['val_loss']))\n",
    "    return min(history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def callback(study, trial):\n",
    "#     if study.best_trial.number == trial.number:\n",
    "#         # Kiểm tra kết quả tốt nhất hiện tại và dừng tìm kiếm nếu đạt được mục tiêu\n",
    "#         if study.best_value < -0.9:\n",
    "#             raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "# study.optimize(objective, n_trials=40, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPTUNA_result(optuna_model):\n",
    "    best_neurons = optuna_model.best_params['n_neurons']\n",
    "    best_activation = optuna_model.best_params['activation']\n",
    "    best_optimizer = optuna_model.best_params['optimizer']\n",
    "    return {\n",
    "        'activations': activation_list[best_activation],\n",
    "        'optimizers': optimizer_list[best_optimizer],\n",
    "        'neurons': best_neurons,\n",
    "        'num_feature': optuna_model.best_params['k_features']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_result = OPTUNA_result(study)\n",
    "best_feature = list(featureScores['Specs'][:study.best_params['k_features']])\n",
    "# pickle.dump(best_feature, open(path_feature + 'best_feature.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_solar = train_scaler_lag[['TotW(t)']+best_feature]\n",
    "val_solar = val_scaler_lag[['TotW(t)']+best_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(train_solar,val_solar,optuna_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27870e2cba32afb35df83c15876cb6bec71e23ef70eb52b377eee57487520364"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
